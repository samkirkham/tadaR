---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  message = FALSE,
  root.dir = "/Users/sam/Dropbox/scripts/github/tadaR")
library(knitr)
```

# tadaR
# R functions for processing and plotting output from TADA (TAsk Dynamics Application)

Sam Kirkham

### Preliminaries ###

The code works directly on the exported `.mat` MATLAB objects from TADA. It requires the following packages to be installed.

```{r}
library(R.matlab)
library(tidyverse)
```

You can load the tadaR functions as follows:

```{r}
source("src/read_tada.R")
source("src/unnest_tada.R")
source("src/getAudioLong.R")
```

### Loading data and accessing variables ###

Load a TADA file using `read_tada` function. As an example, we use a TADA synthesis of the word 'pipe'.

```{r}
d <- read_tada("data/pipe_traj_mv.mat")
```

Now we can plot columns as follows.

```{r}
plot(d$audio$SIGNAL, type = "l") # plot audio signal
```


### Plotting signals and wide data ###

Generally, it's much easier to use the data in an unnested form, with one column for each variable.

```{r}
d.wide <- unnest_tada(d)
```

The data object now looks like this:

```{r}
head(d.wide)
```

We can now refer directly to variables in order to plot the whole signal file. The new object also has a `Time` column.

```{r}
plot(d.wide$Time, d.wide$TBCL, type = "l", xlab = "Time (secs)", ylab = "TBCL")
```

### Long formant data ###

We can also convert the data to long format. This allows us to easily show multiple variables on a single plot, which is very useful for generating something that looks comparable to a gestural score.

First we can create a long data object.

```{r}
d.long <- tidyr::gather(d.wide, "Variable", "Value", -Sample, -Time)
```

In order to get a long audio object, we use `getAudioLong` and add it to the above object. This is necessary because the TADA audio object has a different sampling rate from the articulatory channels.

```{r}
d.long <- dplyr::bind_rows(d.long, getAudioLong(d))
```

The following code shows us which articulators/variables are available to us in `d.long`.

```{r}
unique(d.long$Variable)
```

We can then plot selected variables over time using ggplot. The below code plots the following: Audio, Lip Aperture (LA), TBCL, TBCD, Glottis (GLO).

```{r}
d.long %>% 
  dplyr::filter(Variable %in% c("Audio", "LA", "TBCL", "TBCD", "GLO")) %>% 
  ggplot2::ggplot(aes(x = Time, y = Value)) +
  geom_path() +
  facet_wrap(~Variable, ncol = 1, scales = "free_y") + # let *only* y-scaling be free
  theme_minimal()
```


### Further notes and future plans ###

Coming soon...

1. R package with better documentation and examples.

2. Facility to convert TADA files to SSFF format for use with the EMU system: https://github.com/IPS-LMU/emuR This will allow for interactive viewing of gestural synthesis and use of EMU-webApp's gestural annotation algorithms.

3. Plotting functions for generating gestural scores.

4. Force-alignment of TADA acoustic data to impose segmental boundaries on the gestural score (TADA itself is not segmental, as it synthesises output based on the overlapping gestures)

If you are interested in further articulatory-to-acoustic synthesis using TADA data then HLSyn may also be helpful: https://github.com/samkirkham/hlsyn 